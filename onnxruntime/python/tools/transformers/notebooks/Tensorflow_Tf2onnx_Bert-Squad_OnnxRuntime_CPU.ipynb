{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Inference TensorFlow Bert Model with ONNX Runtime on CPU"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this tutorial, you'll be introduced to how to load a Bert model using TensorFlow, convert it to ONNX using tf2onnx, and inference it for high performance using ONNX Runtime. In the following sections, we are going to use the Bert model trained with Stanford Question Answering Dataset (SQuAD) dataset as an example. Bert SQuAD model is used in question answering scenarios, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0. Prerequisites ##\n",
    "First we need a python environment before running this notebook.\n",
    "\n",
    "You can install [AnaConda](https://www.anaconda.com/distribution/) and [Git](https://git-scm.com/downloads) and open an AnaConda console when it is done. Then you can run the following commands to create a conda environment named cpu_env:\n",
    "\n",
    "```console\n",
    "conda create -n cpu_env python=3.6\n",
    "conda activate cpu_env\n",
    "conda install -c anaconda ipykernel\n",
    "conda install -c conda-forge ipywidgets\n",
    "python -m ipykernel install --user --name=cpu_env\n",
    "```\n",
    "\n",
    "Finally, launch Jupyter Notebook and you can choose cpu_env as kernel to run this notebook.\n",
    "\n",
    "Let's install [Tensorflow](https://www.tensorflow.org/install), [OnnxRuntime](https://microsoft.github.io/onnxruntime/), Keras2Onnx and other packages like the following:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import sys\r\n",
    " \r\n",
    "!{sys.executable} -m pip install --quiet --upgrade tensorflow==2.6.0\r\n",
    "!{sys.executable} -m pip install --quiet --upgrade onnxruntime==1.8.1\r\n",
    "!{sys.executable} -m pip install --quiet --upgrade tf2onnx==1.9.2\r\n",
    "!{sys.executable} -m pip install --quiet transformers==4.9.2\r\n",
    "!{sys.executable} -m pip install --quiet onnxconverter_common\r\n",
    "!{sys.executable} -m pip install --quiet wget pandas"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's define some constants:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Whether allow overwrite existing script or model.\r\n",
    "enable_overwrite = False\r\n",
    "\r\n",
    "# Number of runs to get average latency.\r\n",
    "total_runs = 100\r\n",
    "\r\n",
    "# Max sequence length for the export model\r\n",
    "max_sequence_length = 512"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import os\r\n",
    "cache_dir = './cache_models'\r\n",
    "output_dir = './onnx_models'\r\n",
    "for directory in [cache_dir, output_dir]:\r\n",
    "    if not os.path.exists(directory):\r\n",
    "        os.makedirs(directory)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import tensorflow as tf\r\n",
    "tf.config.set_visible_devices([], 'GPU') # Disable GPU for fair comparison"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Load Pretrained Bert model ##"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Start to load fine-tuned model. This step take a few minutes to download the model for the first time."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from transformers import (TFBertForQuestionAnswering, BertTokenizer)\r\n",
    "\r\n",
    "#model_name_or_path = 'bert-large-uncased-whole-word-masking-finetuned-squad'\r\n",
    "model_name_or_path = \"bert-base-cased\"\r\n",
    "is_fine_tuned = (model_name_or_path == 'bert-large-uncased-whole-word-masking-finetuned-squad')\r\n",
    "\r\n",
    "# Load model and tokenizer\r\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name_or_path, do_lower_case=True, cache_dir=cache_dir)\r\n",
    "model = TFBertForQuestionAnswering.from_pretrained(model_name_or_path, cache_dir=cache_dir)\r\n",
    "# Needed this to export onnx model with multiple inputs with TF 2.2\r\n",
    "model._saved_model_inputs_spec = None"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing TFBertForQuestionAnswering: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of TFBertForQuestionAnswering were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['qa_outputs']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. TensorFlow Inference\n",
    "\n",
    "Use one example to run inference using TensorFlow as baseline."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import numpy\r\n",
    "\r\n",
    "question, text = \"What is ONNX Runtime?\", \"ONNX Runtime is a performance-focused inference engine for ONNX models.\"\r\n",
    "# Pad to max length is needed. Otherwise, position embedding might be truncated by constant folding.\r\n",
    "inputs = tokenizer.encode_plus(question, text, add_special_tokens=True, return_tensors='tf',\r\n",
    "                               max_length=max_sequence_length, pad_to_max_length=True, truncation=True)\r\n",
    "start_scores, end_scores = model(inputs)\r\n",
    "\r\n",
    "num_tokens = len(inputs[\"input_ids\"][0])\r\n",
    "if is_fine_tuned:\r\n",
    "    all_tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\r\n",
    "    print(\"The answer is:\", ' '.join(all_tokens[numpy.argmax(start_scores) : numpy.argmax(end_scores)+1]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import time\r\n",
    "start = time.time()\r\n",
    "for _ in range(total_runs):\r\n",
    "    start_scores, end_scores = model(inputs)\r\n",
    "end = time.time()\r\n",
    "print(\"Tensorflow Inference time for sequence length {} = {} ms\".format(num_tokens, format((end - start) * 1000 / total_runs, '.2f')))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tensorflow Inference time for sequence length 512 = 1133.13 ms\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Export model to ONNX using tf2onnx\r\n",
    "\r\n",
    "Now we use tf2onnx to export the model to ONNX format.\r\n",
    "Note that we could also convert tensorflow checkpoints to pytorch(supported by huggingface team, ref:https://huggingface.co/transformers/converting_tensorflow_models.html) and then convert to onnx using torch.onnx.export()."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "import keras2onnx\r\n",
    "from keras2onnx.proto import keras\r\n",
    "\r\n",
    "output_model_path =  os.path.join(output_dir, 'keras_{}.onnx'.format(model_name_or_path))\r\n",
    "\r\n",
    "if enable_overwrite or not os.path.exists(output_model_path):\r\n",
    "    start = time.time()\r\n",
    "    onnx_model = keras2onnx.convert_keras(model, model.name)\r\n",
    "    keras2onnx.save_model(onnx_model, output_model_path)\r\n",
    "    print(\"Keras2onnx run time = {} s\".format(format(time.time() - start, '.2f')))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Inference the Exported Model with ONNX Runtime"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### OpenMP Environment Variable\n",
    "\n",
    "OpenMP environment variable is important for CPU inference of Bert models. After running this notebook, you can find the best setting from [Performance Test Tool](#Performance-Test-Tool) result for your machine.\n",
    "\n",
    "Setting environment variables shall be done before importing onnxruntime. Otherwise, they might not take effect."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "import os\r\n",
    "import psutil\r\n",
    "\r\n",
    "# ATTENTION: these environment variables must be set before importing onnxruntime.\r\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(psutil.cpu_count(logical=True))\r\n",
    "os.environ[\"OMP_WAIT_POLICY\"] = 'ACTIVE'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we are ready to inference the model with ONNX Runtime. Here we can see that OnnxRuntime has better performance than TensorFlow for this example even without optimization."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "import psutil\r\n",
    "import onnxruntime\r\n",
    "import numpy\r\n",
    "\r\n",
    "sess_options = onnxruntime.SessionOptions()\r\n",
    "\r\n",
    "# intra_op_num_threads=1 can be used to enable OpenMP in OnnxRuntime 1.2.0.\r\n",
    "# For OnnxRuntime 1.3.0 or later, this does not have effect unless you are using onnxruntime-gpu package.\r\n",
    "# sess_options.intra_op_num_threads=1\r\n",
    "\r\n",
    "# Providers is optional. Only needed when you use onnxruntime-gpu for CPU inference.\r\n",
    "session = onnxruntime.InferenceSession(output_model_path, sess_options, providers=['CPUExecutionProvider'])\r\n",
    "\r\n",
    "batch_size = 1\r\n",
    "inputs_onnx = {k_: numpy.repeat(v_, batch_size, axis=0) for k_, v_ in inputs.items()}\r\n",
    "\r\n",
    "# Warm up with one run.\r\n",
    "results = session.run(None, inputs_onnx)\r\n",
    "\r\n",
    "# Measure the latency.\r\n",
    "start = time.time()\r\n",
    "for _ in range(total_runs):\r\n",
    "    results = session.run(None, inputs_onnx)\r\n",
    "end = time.time()\r\n",
    "print(\"ONNX Runtime cpu inference time for sequence length {} (model not optimized): {} ms\".format(num_tokens, format((end - start) * 1000 / total_runs, '.2f')))\r\n",
    "del session"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ONNX Runtime cpu inference time for sequence length 512 (model not optimized): 654.49 ms\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# Some weights of TFBertForQuestionAnswering might not be initialized without fine-tuning.\r\n",
    "if is_fine_tuned:\r\n",
    "    print(\"***** Verifying correctness (TensorFlow and ONNX Runtime) *****\")\r\n",
    "    print('start_scores are close:', numpy.allclose(results[0], start_scores.cpu(), rtol=1e-05, atol=1e-04))\r\n",
    "    print('end_scores are close:', numpy.allclose(results[1], end_scores.cpu(), rtol=1e-05, atol=1e-04))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Model Optimization\n",
    "\n",
    "[ONNX Runtime BERT Model Optimization Tools](https://github.com/microsoft/onnxruntime/tree/master/onnxruntime/python/tools/transformers) is a set of tools for optimizing and testing BERT models. Let's try some of them on the exported models."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### BERT Optimization Script\r\n",
    "\r\n",
    "The script **optimizer.py** can help optimize BERT model exported by PyTorch, tf2onnx or keras2onnx. Since our model is exported by keras2onnx, we shall use **--model_type bert_keras** parameter.\r\n",
    "\r\n",
    "It will also tell whether the model is fully optimized or not. If not, that means you might need change the script to fuse some new pattern of subgraph."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "optimized_model_path =  os.path.join(output_dir, 'keras_{}_opt_cpu.onnx'.format(model_name_or_path))\r\n",
    "\r\n",
    "from onnxruntime_tools import optimizer\r\n",
    "optimized_model = optimizer.optimize_model(output_model_path, model_type='bert_keras', num_heads=12, hidden_size=768)\r\n",
    "optimized_model.use_dynamic_axes()\r\n",
    "optimized_model.save_model_to_file(optimized_model_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We run the optimized model using same inputs. The inference latency is reduced after optimization. The output result is the same as the one before optimization."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "session = onnxruntime.InferenceSession(optimized_model_path, sess_options)\n",
    "# use one run to warm up a session\n",
    "session.run(None, inputs_onnx)\n",
    "\n",
    "# measure the latency.\n",
    "start = time.time()\n",
    "for _ in range(total_runs):\n",
    "    opt_results = session.run(None, inputs_onnx)\n",
    "end = time.time()\n",
    "print(\"ONNX Runtime cpu inference time on optimized model: {} ms\".format(format((end - start) * 1000 / total_runs, '.2f')))\n",
    "del session"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ONNX Runtime cpu inference time on optimized model: 328.48 ms\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "print(\"***** Verifying correctness (before and after optimization) *****\")\n",
    "print('start_scores are close:', numpy.allclose(opt_results[0], results[0], rtol=1e-05, atol=1e-04))\n",
    "print('end_scores are close:', numpy.allclose(opt_results[1], results[1], rtol=1e-05, atol=1e-04))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "***** Verifying correctness (before and after optimization) *****\n",
      "start_scores are close: True\n",
      "end_scores are close: True\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model Results Comparison Tool\n",
    "\n",
    "If your BERT model has three inputs, a script compare_bert_results.py can be used to do a quick verification. The tool will generate some fake input data, and compare results from both the original and optimized models. If outputs are all close, it is safe to use the optimized model.\n",
    "\n",
    "Example of comparing the models before and after optimization:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# The baseline model is exported using max sequence length, and no dynamic axes\n",
    "!{sys.executable} -m onnxruntime_tools.transformers.compare_bert_results --baseline_model $output_model_path --optimized_model $optimized_model_path --batch_size 1 --sequence_length $max_sequence_length --samples 10"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "100% passed for 10 random inputs given thresholds (rtol=0.001, atol=0.0001).\n",
      "maximum absolute difference=1.6242265701293945e-06\n",
      "maximum relative difference=0.009154098108410835\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Performance Test Tool\n",
    "\n",
    "This tool measures performance of BERT model inference using OnnxRuntime Python API.\n",
    "\n",
    "The following command will create 100 samples of batch_size 1 and sequence length 128 to run inference, then calculate performance numbers like average latency and throughput etc."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "THREAD_SETTING = '--intra_op_num_threads 1 --omp_num_threads {} --omp_wait_policy ACTIVE'.format(psutil.cpu_count(logical=True))\n",
    "\n",
    "!{sys.executable} -m onnxruntime_tools.transformers.bert_perf_test --model $optimized_model_path --batch_size 1 --sequence_length 128 --samples 100 --test_times 1 --inclusive $THREAD_SETTING"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Running test: model=keras_bert-base-cased_opt_cpu.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=1,OMP_NUM_THREADS=12,OMP_WAIT_POLICY=ACTIVE,batch_size=1,sequence_length=128,test_cases=100,test_times=1,contiguous=None,use_gpu=False,warmup=True\n",
      "Average latency = 97.93 ms, Throughput = 10.21 QPS\n",
      "test setting TestSetting(batch_size=1, sequence_length=128, test_cases=100, test_times=1, contiguous=None, use_gpu=False, warmup=True, omp_num_threads=12, omp_wait_policy='ACTIVE', intra_op_num_threads=1, seed=3, verbose=False, inclusive=False, extra_latency=True)\n",
      "Generating 100 samples for batch_size=1 sequence_length=128\n",
      "Test summary is saved to onnx_models\\perf_results_CPU_B1_S128_20200728-165907.txt\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's load the summary file and take a look. In this machine, the best result is achieved by OpenMP. The best setting might be difference using different hardware or model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "import glob     \n",
    "import pandas\n",
    "\n",
    "latest_result_file = max(glob.glob(os.path.join(output_dir, \"perf_results_*.txt\")), key=os.path.getmtime)\n",
    "result_data = pandas.read_table(latest_result_file, converters={'OMP_NUM_THREADS': str, 'OMP_WAIT_POLICY':str})\n",
    "print(latest_result_file)\n",
    "\n",
    "result_data.drop(['model', 'graph_optimization_level', 'batch_size', 'sequence_length', 'test_cases', 'test_times', 'use_gpu', 'warmup'], axis=1, inplace=True)\n",
    "result_data.drop(['Latency_P50', 'Latency_P75', 'Latency_P90', 'Latency_P95'], axis=1, inplace=True)\n",
    "cols = result_data.columns.tolist()\n",
    "cols = cols[-4:] + cols[:-4]\n",
    "result_data = result_data[cols]\n",
    "result_data"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "./onnx_models\\perf_results_CPU_B1_S128_20200728-165907.txt\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   intra_op_num_threads OMP_NUM_THREADS OMP_WAIT_POLICY contiguous  \\\n",
       "0                     1              12          ACTIVE       None   \n",
       "\n",
       "   Latency(ms)  Latency_P99  Throughput(QPS)  \n",
       "0        97.93       158.16            10.21  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intra_op_num_threads</th>\n",
       "      <th>OMP_NUM_THREADS</th>\n",
       "      <th>OMP_WAIT_POLICY</th>\n",
       "      <th>contiguous</th>\n",
       "      <th>Latency(ms)</th>\n",
       "      <th>Latency_P99</th>\n",
       "      <th>Throughput(QPS)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>None</td>\n",
       "      <td>97.93</td>\n",
       "      <td>158.16</td>\n",
       "      <td>10.21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Additional Info\n",
    "\n",
    "Note that running Jupyter Notebook has impact on performance result since Jupyter Notebook is using system resources like CPU and memory etc. It is recommended to close Jupyter Notebook and other applications, then run the performance test tool in a console to get more accurate performance numbers.\n",
    "\n",
    "We have a [benchmark script](https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/python/tools/transformers/run_benchmark.sh). It is recommended to use it to measure inference speed of OnnxRuntime.\n",
    "\n",
    "[OnnxRuntime C API](https://github.com/microsoft/onnxruntime/blob/master/docs/C_API.md) could get slightly better performance than python API. If you use C API in inference, you can use OnnxRuntime_Perf_Test.exe built from source to measure performance instead.\n",
    "\n",
    "Here is the machine configuration that generated the above results. The machine has GPU but not used in CPU inference.\n",
    "You might get slower or faster result based on your hardware."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "!{sys.executable} -m onnxruntime_tools.transformers.machine_info --silent"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{\n",
      "  \"gpu\": {\n",
      "    \"driver_version\": \"442.23\",\n",
      "    \"devices\": [\n",
      "      {\n",
      "        \"memory_total\": 8589934592,\n",
      "        \"memory_available\": 8480882688,\n",
      "        \"name\": \"GeForce GTX 1070\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  \"cpu\": {\n",
      "    \"brand\": \"Intel(R) Core(TM) i7-8700 CPU @ 3.20GHz\",\n",
      "    \"cores\": 6,\n",
      "    \"logical_cores\": 12,\n",
      "    \"hz\": \"3.1920 GHz\",\n",
      "    \"l2_cache\": \"1536 KB\",\n",
      "    \"l3_cache\": \"12288 KB\",\n",
      "    \"processor\": \"Intel64 Family 6 Model 158 Stepping 10, GenuineIntel\"\n",
      "  },\n",
      "  \"memory\": {\n",
      "    \"total\": 16971259904,\n",
      "    \"available\": 3480842240\n",
      "  },\n",
      "  \"python\": \"3.6.10.final.0 (64 bit)\",\n",
      "  \"os\": \"Windows-10-10.0.18362-SP0\",\n",
      "  \"onnxruntime\": {\n",
      "    \"version\": \"1.4.0\",\n",
      "    \"support_gpu\": false\n",
      "  },\n",
      "  \"pytorch\": {\n",
      "    \"version\": \"1.5.0+cpu\",\n",
      "    \"support_gpu\": false\n",
      "  },\n",
      "  \"tensorflow\": {\n",
      "    \"version\": \"2.3.0\",\n",
      "    \"git_version\": \"v2.3.0-rc2-23-gb36436b087\",\n",
      "    \"support_gpu\": true\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2020-07-28 16:59:18.638897: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpu_env",
   "language": "python",
   "name": "cpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}